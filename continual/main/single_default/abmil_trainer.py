import os,sys
if "main" in sys.path[0]:
    sys.path[0] = "/".join(sys.path[0].split('/')[:-2])
sys.path = [sys.path[0]] + [f"{sys.path[0]}/ssl_engine"] + sys.path[1:]

from copy import deepcopy
import time
import torch
import wandb
import numpy as np
import torch.nn as nn
from dataloader import *
from torch.utils.data import DataLoader, RandomSampler
import argparse
from configs import get_config
from modules import attmil,clam,dsmil,transmil,mean_max,rrt
import losses

from torch.nn.functional import one_hot

from torch.cuda.amp import GradScaler
from contextlib import suppress
import time

from timm.utils import AverageMeter,dispatch_clip_grad
from timm.models import  model_parameters
from collections import OrderedDict
# from vis.vis_utils import visualize_prediction

from utils import *



class ABMIL_Trainer:
    def __init__(self, args, k, train_p, train_l, test_p, test_l,val_p,val_l):
        self.args = args
        self.organ = args.organ
        self.k =k
        self.train_p = train_p
        self.train_l = train_l
        self.test_p = test_p
        self.test_l = test_l
        self.val_p = val_p
        self.val_l = val_l
        
        self.loss_scaler = GradScaler() if args.amp else None
        self.amp_autocast = torch.cuda.amp.autocast if args.amp else suppress
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        
        self.set_dataset(args, k)
        self.set_data_loader(args)
        self.set_model(args)
        
        if args.loss == 'bce':
            self.criterion = nn.BCEWithLogitsLoss()
            self.criterion_reg=None
        elif args.loss == 'ce':
            self.criterion = nn.CrossEntropyLoss()
            self.criterion_reg=None
    
        self.set_optimizer(args)


    def set_dataset(self, args, k):
        if args.datasets.lower() == 'cm16':
            self.train_set = C16Dataset(self.train_p[k],self.train_l[k],root=args.dataset_root,persistence=args.persistence,keep_same_psize=args.same_psize,is_train=True, patch_labels=True)
            self.test_set = C16Dataset(self.test_p[k],self.test_l[k],root=args.dataset_root,persistence=args.persistence,\
                                                keep_same_psize=args.same_psize, patch_labels=True, return_coords=True)
            
            if args.val_ratio != 0.:
                self.val_set = C16Dataset(self.val_p[k],self.val_l[k],root=args.dataset_root,persistence=args.persistence,\
                                                keep_same_psize=args.same_psize, patch_labels=True, return_coords=True)
            else:
                self.val_set = self.test_set

        elif args.datasets.lower() == 'paip':
            self.train_set = C16Dataset(self.train_p[k],self.train_l[k],root=args.dataset_root,persistence=args.persistence,keep_same_psize=args.same_psize,is_train=True, patch_labels=True)
            self.test_set = C16Dataset(self.test_p[k],self.test_l[k],root=args.dataset_root,persistence=args.persistence,\
                                                keep_same_psize=args.same_psize, patch_labels=True, return_coords=True)
            
            if args.val_ratio != 0.:
                self.val_set = C16Dataset(self.val_p[k],self.val_l[k],root=args.dataset_root,persistence=args.persistence,\
                                                keep_same_psize=args.same_psize, patch_labels=True, return_coords=True)
            else:
                self.val_set = self.test_set

        elif args.datasets.lower() == 'tcga':
            self.train_set = TCGADataset(self.train_p[k],self.train_l[k],args.tcga_max_patch,args.dataset_root,persistence=args.persistence,keep_same_psize=args.same_psize,is_train=True,_type=args.tcga_sub)
            self.test_set = TCGADataset(self.test_p[k],self.test_l[k],args.tcga_max_patch,args.dataset_root,persistence=args.persistence,keep_same_psize=args.same_psize,_type=args.tcga_sub)
            if args.val_ratio != 0.:
                self.val_set = TCGADataset(self.val_p[k],self.val_l[k],args.tcga_max_patch,args.dataset_root,persistence=args.persistence,keep_same_psize=args.same_psize,_type=args.tcga_sub)
            else:
                self.val_set = self.test_set        
            
        return self.train_set, self.val_set, self.test_set,
            
        
    def set_data_loader(self, args):
        # ---> Loading data
        if args.fix_loader_random:
            # generated by int(torch.empty((), dtype=torch.int64).random_().item())
            big_seed_list = 7784414403328510413
            generator = torch.Generator()
            generator.manual_seed(big_seed_list)  
            self.train_loader = DataLoader(self.train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers,generator=generator)
        else:
            self.train_loader = DataLoader(self.train_set, batch_size=args.batch_size, sampler=RandomSampler(self.train_set), num_workers=args.num_workers)

        self.val_loader = DataLoader(self.val_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
        self.test_loader = DataLoader(self.test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)
    
        return self.train_loader, self.val_loader, self.test_loader


    def set_model(self, args):
        # bulid networks

        rrt_enc = None

        if args.model == 'rrtmil':
            model_params = {
                'input_dim': args.input_dim,
                'n_classes': args.n_classes,
                'dropout': args.dropout,
                'act': args.act,
                'region_num': args.region_num,
                'pos': args.pos,
                'pos_pos': args.pos_pos,
                'pool': args.pool,
                'peg_k': args.peg_k,
                'drop_path': args.drop_path,
                'n_layers': args.n_trans_layers,
                'n_heads': args.n_heads,
                'attn': args.attn,
                'da_act': args.da_act,
                'trans_dropout': args.trans_drop_out,
                'ffn': args.ffn,
                'mlp_ratio': args.mlp_ratio,
                'trans_dim': args.trans_dim,
                'epeg': args.epeg,
                'min_region_num': args.min_region_num,
                'qkv_bias': args.qkv_bias,
                'epeg_k': args.epeg_k,
                'epeg_2d': args.epeg_2d,
                'epeg_bias': args.epeg_bias,
                'epeg_type': args.epeg_type,
                'region_attn': args.region_attn,
                'peg_1d': args.peg_1d,
                'cr_msa': args.cr_msa,
                'crmsa_k': args.crmsa_k,
                'all_shortcut': args.all_shortcut,
                'crmsa_mlp':args.crmsa_mlp,
                'crmsa_heads':args.crmsa_heads,
            }
            model = rrt.RRTMIL(**model_params).to(self.device)



        elif args.model == 'attmil':
            model = attmil.DAttention(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)
        elif args.model == 'gattmil':
            model = attmil.AttentionGated(input_dim=args.input_dim,dropout=args.dropout,rrt=rrt_enc).to(self.device)
        elif args.model == 'ibmil':
            if not args.confounder_path.endswith('.npy'):
                _confounder_path = os.path.join(args.confounder_path,str(self.k),'train_bag_cls_agnostic_feats_proto_'+str(args.confounder_k)+'.npy')
            else:
                _confounder_path =args.confounder_path
            model = attmil_ibmil.Dattention_ori(out_dim=args.n_classes,dropout=args.dropout,in_size=args.input_dim,confounder_path=_confounder_path,rrt=rrt_enc).to(self.device)
        # follow the official code
        # ref: https://github.com/mahmoodlab/CLAM
        elif args.model == 'clam_sb':
            model = clam.CLAM_SB(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)
        elif args.model == 'clam_mb':
            model = clam.CLAM_MB(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)
        elif args.model == 'transmil':
            model = transmil.TransMIL(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act).to(self.device)
        elif args.model == 'dsmil':
            model = dsmil.MILNet(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)
            args.cls_alpha = 0.5
            args.aux_alpha = 0.5
            state_dict_weights = torch.load('./modules/mil_modules/init_ckp/dsmil_init.pth')
            info = model.load_state_dict(state_dict_weights, strict=False)
            if not args.no_log:
                print(info)
        elif args.model == 'meanmil':
            model = mean_max.MeanMIL(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)
        elif args.model == 'maxmil':
            model = mean_max.MaxMIL(input_dim=args.input_dim,n_classes=args.n_classes,dropout=args.dropout,act=args.act,rrt=rrt_enc).to(self.device)

        self.model = model

        ############################################
        ########## load pre-trained model ##########
        
        if not args.wo_pretrain:
            ckp_pretrain = torch.load(args.pretrain_model_path)
            self.model.load_state_dict(ckp_pretrain['model'])
        ########## load pre-trained model ##########
        ############################################


    def set_ema_model(self):
        pass

    def set_optimizer(self, args):
        # optimizer
        if args.opt == 'adamw':
            self.optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=args.lr, weight_decay=args.weight_decay)
        elif args.opt == 'adam':
            self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=args.lr, weight_decay=args.weight_decay)

        if args.lr_sche == 'cosine':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, args.num_epoch, 0) \
                            if not args.lr_supi else torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, args.num_epoch*len(self.train_loader), 0)
        elif args.lr_sche == 'cosine_restart':
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, args.num_epoch//args.num_restart_cycles, 1, 0)
        elif args.lr_sche == 'step':
            assert not args.lr_supi
            # follow the DTFD-MIL
            # ref:https://github.com/hrzhang1123/DTFD-MIL
            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,args.num_epoch / 2, 0.2)
        elif args.lr_sche == 'const':
            self.scheduler = None

        if args.early_stopping:
            self.early_stopping = EarlyStopping(patience=30 if args.datasets=='camelyon16' else 20, \
                                            stop_epoch=args.max_epoch if args.datasets=='camelyon16' else 70,
                                            save_best_model_stage=np.ceil(args.save_best_model_stage * args.num_epoch))
        else:
            self.early_stopping = None

    def compute_prob(self, logits):
        return torch.softmax(logits, dim=-1)
    
    def train(self):      
        acs,pre,rec,fs,auc,te_auc,te_fs=[],[],[],[],[],[],[]
        mious,dices,acs_patch,fs_patch,auc_patch,ths_patch=[],[],[],[],[],[]


        optimal_ac, opt_pre, opt_re, opt_fs, opt_auc,opt_epoch = 0,0,0,0,0,0
        opt_miou, opt_dice, opt_acc_patch, opt_auc_patch, opt_fs_patch = 0,0,0,0,0
        epoch_start = 0

        if self.args.fix_train_random:
            seed_torch(self.args.seed)

        train_time_meter = AverageMeter()
        for epoch in range(epoch_start, self.args.num_epoch):
            train_loss,start,end = self.train_loop(self.args,self.model,self.train_loader,\
                                            self.optimizer,self.device,self.amp_autocast,\
                                            self.criterion,self.loss_scaler,self.scheduler,\
                                            self.k, epoch, self.criterion_reg)
                    
            train_time_meter.update(end-start)

            default_wsi_path = deepcopy(self.args.wsi_path)

            self.args.wsi_path = default_wsi_path.replace("#organ#", self.organ)


            if epoch%self.args.val_interval==0 or epoch == self.args.num_epoch-1:
                stop, accuracy, auc_value, precision, recall, fscore, count_tumor_slide, miou, dice, acc_patch, auc1_patch, fscore_patch, th_patch, test_loss, vis_data = \
                                self.val_loop(self.args,self.model,self.val_loader,self.device,self.criterion,self.early_stopping,epoch)

                acs.append(accuracy)
                pre.append(precision)
                rec.append(recall)
                fs.append(fscore)
                auc.append(auc_value)
                mious.append(miou)
                dices.append(dice)
                acs_patch.append(acc_patch)
                ths_patch.append(th_patch)
                fs_patch.append(fscore_patch)
                auc_patch.append(auc1_patch)            

                if not self.args.no_log:
                    print('\r Epoch [%d/%d] train loss: %.1E, test loss: %.1E, accuracy: %.3f, auc_value:%.3f, precision: %.3f, recall: %.3f, fscore: %.3f, miou: %.3f, dice: %.3f, time: %.3f(%.3f)' % 
                (epoch+1, self.args.num_epoch, train_loss, test_loss, accuracy, auc_value, precision, recall, fscore, miou, dice, train_time_meter.val,train_time_meter.avg))



                #####################################################
                ################### log to wandb ####################
                if vis_data is not None:
                    img_log_dict = {}

                    for img, wsi_name in zip(vis_data[-2], vis_data[-1]):
                        img_log_dict[str(self.k)+'-fold_image/'+wsi_name] = wandb.Image(img)

                    wandb.log(img_log_dict)

                if self.args.wandb:
                    rowd = OrderedDict([
                        ("val_acc",accuracy),
                        ("val_precision",precision),
                        ("val_recall",recall),
                        ("val_fscore",fscore),
                        ("val_auc",auc_value),
                        ("val_miou",miou),
                        ("val_dice",dice),
                        ("val_acc_patch",acc_patch),
                        ("val_ths_patch", th_patch),
                        ("val_fs_patch", fscore_patch),
                        ("val_auc_patch", auc1_patch),
                        ("val_loss",test_loss),
                        ("epoch",epoch),
                    ])

                    rowd = OrderedDict([ (str(self.k)+'-fold/'+_k,_v) for _k, _v in rowd.items()])
                    wandb.log(rowd,commit=False)
                ################### log to wandb ####################
                #####################################################


                ###################################################################
                ################### log best metrics and model ####################
                if dice > opt_dice and epoch >= self.args.save_best_model_stage*self.args.num_epoch:
                    optimal_ac = accuracy
                    opt_pre = precision
                    opt_re = recall
                    opt_fs = fscore
                    opt_auc = auc_value
                    opt_epoch = epoch
                    opt_miou = miou
                    opt_dice = dice
                    opt_acc_patch = acc_patch
                    opt_auc_patch = auc1_patch
                    opt_fs_patch = fscore_patch 

                    best_model_state_dict = self.model.state_dict()


                    ckp = {
                        'model': best_model_state_dict,
                        'k': self.k,
                    }

                    ckc_metric = [acs,auc,pre,rec,fs,mious,dices,acc_patch,auc_patch,fs_patch,ths_patch,te_auc,te_fs]

                    results = {"ckc_metric": ckc_metric,
                                "k": self.k,
                                "epoch": epoch
                            }
                    if self.args.task_setup in ["single", "joint"]:
                        os.makedirs(self.args.model_path, exist_ok=True)
                        torch.save(ckp, os.path.join(self.args.model_path, f'fold_{self.k}_best_model.pt'))
                        torch.save(results, os.path.join(self.args.model_path, f'fold_{self.k}_ckc_metric.pt'))

                    elif self.args.task_setup in ["continual"]:
                        os.makedirs(self.args.model_path, exist_ok=True)
                        torch.save(ckp, os.path.join(self.args.model_path, f'fold_{self.k}_task_{self.current_task}_best_model.pt'))
                        torch.save(results, os.path.join(self.args.model_path, f'fold_{self.k}_task_{self.current_task}_ckc_metric.pt'))                        


                if self.args.wandb:
                    rowd = OrderedDict([
                        ("val_best_acc",optimal_ac),
                        ("val_best_precesion",opt_pre),
                        ("val_best_recall",opt_re),
                        ("val_best_fscore",opt_fs),
                        ("val_best_auc",opt_auc),
                        ("val_best_epoch",opt_epoch),
                        ("val_best_miou",opt_miou),
                        ("val_best_dice",opt_dice),   
                        ("val_best_acc_patch",opt_acc_patch),
                        ("val_best_auc_patch",opt_auc_patch),
                        ("val_best_fs_patch",opt_fs_patch),                                      
                    ])

                    rowd = OrderedDict([ (str(self.k)+'-fold_opt/'+_k,_v) for _k, _v in rowd.items()])
                    wandb.log(rowd)

                ################### log best metrics and model ####################
                ###################################################################                    

                ckc_metric = [acs,auc,pre,rec,fs,mious,dices,acc_patch,auc_patch,fs_patch,ths_patch,te_auc,te_fs]
                results = {"ckc_metric": ckc_metric,
                            "k": self.k,
                        }



        if self.args.task_setup in ["single", "joint"]:
            os.makedirs(self.args.model_path, exist_ok=True)
            torch.save(ckp, os.path.join(self.args.model_path, f'fold_{self.k}_final_model.pt'))
            torch.save(results, os.path.join(self.args.model_path, f'fold_{self.k}_final_ckc_metric.pt'))

        elif self.args.task_setup in ["continual"]:
            os.makedirs(self.args.model_path, exist_ok=True)
            torch.save(ckp, os.path.join(self.args.model_path, f'fold_{self.k}_task_{self.current_task}_final_model.pt'))
            torch.save(results, os.path.join(self.args.model_path, f'fold_{self.k}_task_{self.current_task}_final_ckc_metric.pt'))                        



        


    def train_loop(self, args,model,loader,optimizer,device,amp_autocast,criterion,loss_scaler,scheduler,k,epoch, criterion_reg=None):
        start = time.time()
        loss_cls_meter = AverageMeter()
        loss_cl_meter = AverageMeter()
        patch_num_meter = AverageMeter()
        keep_num_meter = AverageMeter()

        train_loss_log = 0.
        model.train()
        

        train_acc = 0.
        n_slides = 0.
        for i, data in enumerate(loader):
            optimizer.zero_grad()

            if isinstance(data[0],(list,tuple)):
                for i in range(len(data[0])):
                    data[0][i] = data[0][i].to(device)
                bag=data[0]
                batch_size=data[0][0].size(0)
            else:
                bag=data[0].to(device)  # b*n*1024
                batch_size=bag.size(0)
                
            label=data[1].to(device)
            patch_labels = data[2].squeeze(0).squeeze(-1).to(device)        
                    
            with amp_autocast():
                ###############################################
                ############## Bag augmentation ###############
                if args.patch_shuffle:
                    bag = patch_shuffle(bag,args.shuffle_group)
                elif args.group_shuffle:
                    bag = group_shuffle(bag,args.shuffle_group)
                ############## Bag augmentation ###############
                ###############################################

                ###############################################
                ############### Model forward #################

                if args.model in ('clam_sb','clam_mb','dsmil'):
                    train_logits,cls_loss,patch_num,train_patch_attn = model(bag,label,criterion)                    
                    train_patch_attn = train_patch_attn[:, 1:]
                    logit_min, _ = train_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = train_patch_attn.max(dim=0, keepdim=True)
                    train_patch_prob = (train_patch_attn-logit_min) / (logit_max+1e-10)
                    train_patch_prob = torch.cat([1-train_patch_prob, train_patch_prob],dim=1)
                    keep_num = patch_num

                elif args.model in ('attmil', 'transmil', 'rrtmil', 'cdatmil'):
                    train_logits, train_patch_attn = model(bag,return_attn=True,no_norm=False)                    
                    train_patch_attn = train_patch_attn.permute(1,0)
                    logit_min, _ = train_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = train_patch_attn.max(dim=0, keepdim=True)
                    train_patch_prob = (train_patch_attn-logit_min) / (logit_max+1e-10)
                    train_patch_prob = torch.cat([1-train_patch_prob, train_patch_prob],dim=1)
                    cls_loss,patch_num,keep_num = 0.,0.,0.       


                elif "AgentMIL" in model.__class__.__name__:
                    train_logits, train_patch_attn = model(bag, return_attn=True, train_mode=True)      
                    train_patch_attn = train_patch_attn.permute(1,0)
                    logit_min, _ = train_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = train_patch_attn.max(dim=0, keepdim=True)
                    train_patch_prob = (train_patch_attn-logit_min) / (logit_max+1e-10)
                    train_patch_prob = torch.cat([1-train_patch_prob, train_patch_prob],dim=1)
                    cls_loss,patch_num,keep_num = 0.,0.,0.           

                ############### Model forward #################
                ###############################################


                # if self.current_task > 0:
                #     breakpoint()

                ###############################################
                ################ Compute loss #################
                if args.loss == 'ce':
                    logit_loss = criterion(train_logits.view(batch_size,-1),label)
                    val, pred = torch.softmax(train_logits.detach(), dim=1).max(dim=1)
                    patch_log_softmax= torch.log(train_patch_prob+1e-10) 

                    patch_logit_loss = F.nll_loss(patch_log_softmax, patch_labels%2)                    

                    n_slides += pred.size(0)
                    if i == 0:
                        train_acc = (label == pred).float().sum() / n_slides
                    else:
                        train_acc = ((n_slides - pred.size(0)) / n_slides) * train_acc + (1/n_slides) * (label == pred).float().sum() 

                elif args.loss == 'bce':
                    logit_loss = criterion(train_logits.view(batch_size,-1),one_hot(label.view(batch_size,-1).float(),num_classes=2))
                    patch_logit_loss = criterion(train_patch_prob, one_hot(patch_labels, num_classes=2))

                train_loss = args.cls_alpha * logit_loss +  cls_loss*args.aux_alpha


                ########### Include instance loss after wamup iters ###########
                if epoch >= args.train_patch_epoch:
                    train_loss += args.seg_coef * patch_logit_loss

                train_loss = train_loss / args.accumulation_steps
                ################ Compute loss #################
                ###############################################


            if pred.size(0) > 1:
                if i % 25 == 0:
                    print(f"[{i}/{len(loader)}] label: {label}, logit loss: {args.cls_alpha * logit_loss:15.5}, patch logit loss: {args.seg_coef * patch_logit_loss:15.5}, pred: {pred}, conf: {val}, train_acc: {train_acc}")
            else:
                if i % 25 == 0:
                    print(f"[{i}/{len(loader)}] label: {label.item()}, logit loss: {args.cls_alpha * logit_loss:15.5}, patch logit loss: {args.seg_coef * patch_logit_loss:15.5}, pred: {pred.item()}, conf: {val.item():15.5}, train_acc: {train_acc:15.5}")
    

            ######################################################
            ##################### optimize #######################
            if args.clip_grad > 0.:
                dispatch_clip_grad(
                    model_parameters(model),
                    value=args.clip_grad, mode='norm')

            if (i+1) % args.accumulation_steps == 0:
                train_loss.backward()


                optimizer.step()
                if args.lr_supi and scheduler is not None:
                    scheduler.step()
            ##################### optimize #######################
            ######################################################



            ###########################################
            ################# Logging #################
            loss_cls_meter.update(logit_loss,1)
            loss_cl_meter.update(cls_loss,1)
            patch_num_meter.update(patch_num,1)
            keep_num_meter.update(keep_num,1)

            if (i+1) % args.log_iter == 0 or i == len(loader)-1:
                lrl = [param_group['lr'] for param_group in optimizer.param_groups]
                lr = sum(lrl) / len(lrl)
                rowd = OrderedDict([
                    ('cls_loss',loss_cls_meter.avg),
                    ('lr',lr),
                    ('cl_loss',loss_cl_meter.avg),
                    ('patch_num',patch_num_meter.avg),
                    ('keep_num',keep_num_meter.avg),
                    ('train_slide_acc',train_acc),
                ])

                rowd = OrderedDict([ (str(k)+'-fold/'+_k,_v) for _k, _v in rowd.items()])
                if args.wandb:
                    wandb.log(rowd)

            train_loss_log = train_loss_log + train_loss.item()
            ################# Logging #################
            ###########################################


        end = time.time()
        train_loss_log = train_loss_log/len(loader)
        if not args.lr_supi and scheduler is not None:
            scheduler.step()
        
        return train_loss_log,start,end


    def val_loop(self, args,model,loader,device,criterion,early_stopping,epoch,labels=[0,1],th_patch=None):
        model.eval()
        loss_cls_meter = AverageMeter()
        test_loss_log = 0.
        bag_logit, bag_labels=[], []
        
        bag_logit_patch, bag_labels_patch = [],[]
        bag_coords, bag_wsi_names = [], []

        
        # pred= []
        with torch.no_grad():
            for i, data in enumerate(loader):
                if len(data[1]) > 1:
                    bag_labels.extend(data[1])
                    bag_labels_patch += [data[2][i] for i in range(len(data[1]))]
                    bag_coords += [data[3][i] for i in range(len(data[3]))]
                    bag_wsi_names += [data[4][i] for i in range(len(data[4]))]
                    
                else:
                    bag_labels.append(data[1])
                    bag_labels_patch.append(data[2])
                    bag_coords.append(data[3])
                    bag_wsi_names.append(data[4])                

                if isinstance(data[0],(list,tuple)):
                    for i in range(len(data[0])):
                        data[0][i] = data[0][i].to(device)
                    bag=data[0]
                    batch_size=data[0][0].size(0)
                else:
                    bag=data[0].to(device)  # b*n*1024
                    batch_size=bag.size(0)

                label=data[1].to(device)
                

                if args.model == 'dsmil':
                    test_logits,cls_loss,patch_num,test_patch_attn = model(bag,label,criterion)
                    test_patch_attn = test_patch_attn[:, 1:]
                    logit_min, _ = test_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = test_patch_attn.max(dim=0, keepdim=True)
                    test_patch_prob = (test_patch_attn-logit_min) / (logit_max+1e-10)
                    test_patch_prob = torch.cat([1-test_patch_prob, test_patch_prob],dim=1)

                    keep_dim=patch_num


                elif args.model in ["attmil", "transmil", "rrtmil", "cdatmil"]:
                    test_logits, test_patch_attn = model(bag,return_attn=True,no_norm=False)
                    test_patch_attn = test_patch_attn.permute(1,0)
                    logit_min, _ = test_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = test_patch_attn.max(dim=0, keepdim=True)
                    test_patch_prob = (test_patch_attn-logit_min) / (logit_max+1e-10)
                    test_patch_prob = torch.cat([1-test_patch_prob, test_patch_prob],dim=1)
                    cls_loss,patch_num,keep_num = 0.,0.,0.       


                elif "AgentMIL" in model.__class__.__name__:
                    test_logits, test_patch_attn = model(bag, return_attn=True, train_mode=False)      
                    test_patch_attn = test_patch_attn.permute(1,0)
                    logit_min, _ = test_patch_attn.min(dim=0, keepdim=True)
                    logit_max, _ = test_patch_attn.max(dim=0, keepdim=True)
                    test_patch_prob = (test_patch_attn-logit_min) / (logit_max+1e-10)
                    test_patch_prob = torch.cat([1-test_patch_prob, test_patch_prob],dim=1)
                    cls_loss,patch_num,keep_num = 0.,0.,0.       





                else:
                    test_logits, test_patch_logits = model(bag, return_patch_logits=True)

                if args.loss == 'ce':
                    if args.model in ["attmil", "dsmil", "transmil", "rrtmil", "agentmil", "cdatmil"]:
                        test_loss = criterion(test_logits.view(batch_size,-1),label)
                        if batch_size > 1:
                            bag_logit.extend(torch.softmax(test_logits,dim=-1).cpu().squeeze())
                        else:
                            bag_logit.append(torch.softmax(test_logits,dim=-1).cpu().squeeze())

                        slide_labels_to_patch = (label*torch.ones(test_patch_prob.size(0)).to(device)).long()
                        patch_log_softmax= torch.log(test_patch_prob+1e-10) 
                        patch_logit_loss = F.nll_loss(patch_log_softmax, slide_labels_to_patch%2)
                        test_loss += args.seg_coef * patch_logit_loss

                        if batch_size > 1:
                            bag_logit_patch += [test_patch_prob.cpu() for i in range(batch_size)]
                        else:
                            bag_logit_patch.append(test_patch_prob.cpu())
                    
                    else:
                        test_loss = criterion(test_logits.view(batch_size,-1),label)
                        if batch_size > 1:
                            bag_logit.extend(torch.softmax(test_logits,dim=-1).cpu().squeeze())
                        else:
                            bag_logit.append(torch.softmax(test_logits,dim=-1).cpu().squeeze())
                        
                        slide_labels_to_patch = (label*torch.ones(test_patch_logits.size(0)).to(device)).long()
                        patch_logit_loss = criterion(test_patch_logits, slide_labels_to_patch)
                        test_loss += args.seg_coef * patch_logit_loss

                        if batch_size > 1:
                            bag_logit_patch += [torch.softmax(test_patch_logits[i], dim=-1).cpu() for i in range(batch_size)]
                        else:
                            bag_logit_patch.append(torch.softmax(test_patch_logits, dim=-1).cpu())

                loss_cls_meter.update(test_loss,1)
        
        subtyping = (args.datasets.lower() == "tcga")  

        if self.args.task_setup in ["single", "joint"]:
            labels = list(range(self.args.n_classes))
        elif self.args.task_setup in ["continual"]:
            labels = self.labels

        # breakpoint()

        accuracy, auc_value, precision, recall, fscore, count_tumor_slide, miou, dice, acc_patch, auc1_patch, fscore_patch, th_patch, bag_predictions_patch = \
                                    five_scores_and_seg_scores(bag_labels, bag_logit, bag_labels_patch, bag_logit_patch, subtyping, return_pred_patch=True, labels=labels,th_patch=th_patch)    
        
        # breakpoint()

        vis_data = None
        if (not self.args.disable_vis_seg) and (epoch % args.per_seg_vis_epoch == 0):
            vis_data = visualize_prediction(bag_labels_patch, bag_predictions_patch, bag_coords,\
                        bag_wsi_names,wsi_path=args.wsi_path, vis_sample_interval=args.vis_sample_interval)

        # early stop
        if early_stopping is not None:
            early_stopping(epoch,-auc_value,model)
            stop = early_stopping.early_stop
        else:
            stop = False
        return stop,accuracy, auc_value, precision, recall, fscore, count_tumor_slide, miou, dice, acc_patch, auc1_patch, fscore_patch, th_patch, loss_cls_meter.avg, vis_data
    

    def test(self, args,model,loader,device,criterion,labels=[0,1]):
        model.eval()
        test_loss_log = 0.
        bag_logit, bag_labels=[], []
        bag_logit_patch, bag_labels_patch = [],[]

        with torch.no_grad():
            for i, data in enumerate(loader):
                if len(data[1]) > 1:
                    bag_labels.extend(data[1].tolist())
                    bag_labels_patch += [data[2][i] for i in range(len(data[1]))]
                else:
                    bag_labels.append(data[1].item())
                    bag_labels_patch.append(data[2])
                    
                    
                    
                if isinstance(data[0],(list,tuple)):
                    for i in range(len(data[0])):
                        data[0][i] = data[0][i].to(device)
                    bag=data[0]
                    batch_size=data[0][0].size(0)
                else:
                    bag=data[0].to(device)  # b*n*1024
                    batch_size=bag.size(0)

                label=data[1].to(device)

                if args.model == 'dsmil':
                    test_logits,_ = model(bag)
                else:                
                    test_logits, test_patch_logits = model(bag, return_patch_logits=True)
                    

                if args.loss == 'ce' or args.loss == 'proxy_anchor':
                    if (args.model == 'dsmil' and args.ds_average) or (args.model == 'mhim' and isinstance(test_logits,(list,tuple))):
                        test_loss = criterion(test_logits[0].view(batch_size,-1),label)
                        bag_logit.append((0.5*torch.softmax(test_logits[1],dim=-1)+0.5*torch.softmax(test_logits[0],dim=-1))[:,1].cpu().squeeze().numpy())
                    else:
                        test_loss = criterion(test_logits.view(batch_size,-1),label)
                        if batch_size > 1:
                            bag_logit.extend(torch.softmax(test_logits,dim=-1).cpu().squeeze().numpy())
                        else:
                            bag_logit.append(torch.softmax(test_logits,dim=-1).cpu().squeeze().numpy())
                        
                        slide_labels_to_patch = (label*torch.ones(test_patch_logits.size(0)).to(device)).long()
                        patch_logit_loss = criterion(test_patch_logits, slide_labels_to_patch)
                        test_loss += args.seg_coef * patch_logit_loss

                        if batch_size > 1:
                            bag_logit_patch += [torch.softmax(test_patch_logits[i], dim=-1).cpu().numpy() for i in range(batch_size)]
                        else:
                            bag_logit_patch.append(torch.softmax(test_patch_logits, dim=-1).cpu().numpy())
                                
                elif args.loss == 'bce':
                    if args.model == 'dsmil' and args.ds_average:
                        test_loss = criterion(test_logits[0].view(batch_size,-1),label)
                        bag_logit.append((0.5*torch.sigmoid(test_logits[1])+0.5*torch.sigmoid(test_logits[0]).cpu().squeeze().numpy()))
                    else:
                        test_loss = criterion(test_logits.view(batch_size,-1),label.view(1,-1).float())
                        bag_logit.append(torch.sigmoid(test_logits).cpu().squeeze().numpy())    

                        slide_labels_to_patch = (label*torch.ones(test_patch_logits.size(1)).to(device)).long()
                        patch_logit_loss = criterion(test_patch_logits.squeeze(0), one_hot(slide_labels_to_patch, num_classes=2))
                        test_loss += args.seg_coef * patch_logit_loss                    
                        bag_logit_patch.append(torch.sigmoid(test_patch_logits).cpu().squeeze().numpy())                            
                    
            
                test_loss_log = test_loss_log + test_loss.item()

        
        subtyping = (args.datasets.lower() == "tcga")  
        accuracy, auc_value, precision, recall, fscore, count_tumor_slide, miou, dice = \
                                    five_scores_and_seg_scores(bag_labels, bag_logit, bag_labels_patch, bag_logit_patch, subtyping, labels=labels)
            


        test_loss_log = test_loss_log/len(loader)

        return accuracy, auc_value, precision, recall, fscore, count_tumor_slide, miou, dice, test_loss_log

